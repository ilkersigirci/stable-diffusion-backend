{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilker/Documents/MyRepos/stable-diffusion-backend/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from stable_diffusion_backend.ml.image_generation import text_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a realistic happy dog playing in the grass\"\n",
    "\n",
    "negative_prompt = None\n",
    "# negative_prompt = \"render, cartoon, cgi, render, illustration, painting, drawing\"\n",
    "# negative_prompt = \"cgi, 3d render, bad quality, worst quality, text, signature, watermark, extra limbs\"\n",
    "\n",
    "seed = None\n",
    "# seed = 2084045068\n",
    "\n",
    "# clip_skip = None\n",
    "clip_skip = 1\n",
    "# clip_skip = 2\n",
    "\n",
    "image = text_to_img(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    seed=seed,\n",
    "    num_inference_steps=7,\n",
    "    cfg_scale=2.0,\n",
    "    clip_skip=clip_skip,\n",
    ")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Images\n",
    "\n",
    "- [Diffusers Issue](https://github.com/huggingface/diffusers/issues/3579)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "from base64 import b64encode\n",
    "from io import BytesIO\n",
    "\n",
    "import discord\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Basic libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\n",
    "\n",
    "# matplotlib inline\n",
    "## For video display\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "## disable warnings\n",
    "# logging.disable(logging.WARNING)\n",
    "## Imaging  library\n",
    "from PIL import Image\n",
    "from torchvision import transforms as tfms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## Import the CLIP artifacts\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "## Initiating tokenizer and encoder.\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    \"openai/clip-vit-large-patch14\", torch_dtype=torch.float16\n",
    ")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    \"openai/clip-vit-large-patch14\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "## Initiating the VAE\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "## Initializing a scheduler and Setting number of sampling steps\n",
    "scheduler = LMSDiscreteScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    num_train_timesteps=1000,\n",
    ")\n",
    "scheduler.set_timesteps(50)\n",
    "## Initializing the U-Net model\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "## Helper functions\n",
    "def load_image(p):\n",
    "    \"\"\"\n",
    "    Function to load images from a defined path\n",
    "    \"\"\"\n",
    "    return Image.open(p).convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "\n",
    "def pil_to_latents(image):\n",
    "    \"\"\"\n",
    "    Function to convert image to latents\n",
    "    \"\"\"\n",
    "    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n",
    "    init_image = init_image.to(device=\"cuda\", dtype=torch.float16)\n",
    "    return vae.encode(init_image).latent_dist.sample() * 0.18215\n",
    "\n",
    "\n",
    "def latents_to_pil(latents):\n",
    "    \"\"\"\n",
    "    Function to convert latents to images\n",
    "    \"\"\"\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    return [Image.fromarray(image) for image in images]\n",
    "\n",
    "\n",
    "def text_enc(prompts, maxlen=None):\n",
    "    \"\"\"\n",
    "    A function to take a texual promt and convert it into embeddings\n",
    "    \"\"\"\n",
    "    if maxlen is None:\n",
    "        maxlen = tokenizer.model_max_length\n",
    "    inp = tokenizer(\n",
    "        prompts,\n",
    "        padding=\"max_length\",\n",
    "        max_length=maxlen,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()\n",
    "\n",
    "\n",
    "async def prompt_2_img(\n",
    "    prompt,\n",
    "    g=7.5,\n",
    "    batch_size=1,\n",
    "    seed=0,\n",
    "    steps=70,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    message=None,\n",
    "    context=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Diffusion process to convert prompt to image\n",
    "    \"\"\"\n",
    "    warning_step = 0\n",
    "\n",
    "    # Converting textual prompts to embedding\n",
    "    text = text_enc([prompt] * batch_size)\n",
    "\n",
    "    # Adding an unconditional prompt , helps in the generation process\n",
    "    uncond = text_enc([\"\"] * batch_size, text.shape[1])\n",
    "    emb = torch.cat([uncond, text])\n",
    "\n",
    "    # Setting the seed\n",
    "    if seed:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # Initiating random noise\n",
    "    latents = torch.randn((batch_size, unet.in_channels, height // 8, width // 8))\n",
    "\n",
    "    # Setting number of steps in scheduler\n",
    "    scheduler.set_timesteps(steps)\n",
    "\n",
    "    # Adding noise to the latents\n",
    "    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n",
    "\n",
    "    # Iterating through defined steps\n",
    "    for i, ts in enumerate(tqdm(scheduler.timesteps)):\n",
    "        # We need to scale the i/p latents to match the variance\n",
    "        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "\n",
    "        # Predicting noise residual using U-Net\n",
    "        with torch.no_grad():\n",
    "            u, t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n",
    "\n",
    "        # Performing Guidance\n",
    "        pred = u + g * (t - u)\n",
    "\n",
    "        # Conditioning  the latents\n",
    "        latents = scheduler.step(pred, ts, latents).prev_sample\n",
    "\n",
    "        # Saving intermediate images\n",
    "        if context.author and message.content:\n",
    "            if warning_step == i:\n",
    "                images = latents_to_pil(latents)\n",
    "                canvas = Image.new(\"RGBA\", (width * 2, height * 2), (255, 255, 255, 0))\n",
    "                if len(images) >= 1:\n",
    "                    canvas.paste(images[0], (0, 0))\n",
    "                if len(images) >= 2:\n",
    "                    canvas.paste(images[1], (width, 0))\n",
    "                if len(images) >= 3:\n",
    "                    canvas.paste(images[2], (0, height))\n",
    "                if len(images) == 4:\n",
    "                    canvas.paste(images[3], (width, height))\n",
    "                image_bytes = BytesIO()\n",
    "                # Save the image data to the BytesIO object\n",
    "                canvas.save(image_bytes, format=\"PNG\")\n",
    "                image_bytes.seek(0)\n",
    "                file = discord.File(image_bytes, filename=f\"steps_{i:04}.png\")\n",
    "                await message.edit(\n",
    "                    content=f\"**{prompt}** - {context.author.mention} ({i%steps}%) ({'fast' if steps <= 70 else 'slow'})\",\n",
    "                    attachments=[file],\n",
    "                )\n",
    "                warning_step += 10\n",
    "\n",
    "    # Returning the latent representation to output an image of 3x512x512\n",
    "    return latents_to_pil(latents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
